{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# navigates to the main project folder\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    \n",
    "from src.behavior_cloning import *\n",
    "from src.utils.data_loading import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Loading Data </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb_observations, rb_next_observations, rb_actions, rb_rewards, rb_dones = load_data()\n",
    "fp_observations, fp_next_observations, fp_actions, fp_rewards, fp_dones = load_data('../data/final_policy.npz')\n",
    "rb_df = load_data_as_df(rb_observations, rb_next_observations, rb_actions, rb_rewards, rb_dones)\n",
    "fp_df = load_data_as_df(fp_observations, fp_next_observations, fp_actions, fp_rewards, fp_dones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Implementations </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BC_data_loaders(observations: np.array, actions: np.array,\n",
    "                        train: float = 0.70,\n",
    "                        test: float = 0.15,\n",
    "                        validation: float = 0.15,\n",
    "                        batch_size: int = 32,\n",
    "                        seed: int = 16) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    assert abs(train + test + validation - 1.0) < 1e-5, 'Data splits must add up to 1.'\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    test_to_valid_ratio = test / (test + validation)\n",
    "\n",
    "    observations_train, observations_test_valid, actions_train, actions_test_valid = train_test_split(\n",
    "        observations, actions, test_size=(1 - train), random_state=seed)\n",
    "    observations_test, observations_valid, actions_test, actions_valid = train_test_split(\n",
    "        observations, actions, test_size=test_to_valid_ratio, random_state=seed)\n",
    "\n",
    "    train_dataset = BCDataset(observations_train, actions_train)\n",
    "    test_dataset = BCDataset(observations_test, actions_test)\n",
    "    valid_dataset = BCDataset(observations_valid, actions_valid)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(train_loader: DataLoader, val_loader: DataLoader, optimizer: torch.optim.Optimizer, model: torch.nn.Module,\n",
    "        early_stop_epoch_without_improvement: int = 3, loss_function: callable = torch.nn.CrossEntropyLoss(), epochs=6, log_subfolder: str = 'logs'):\n",
    "    tensorboard_log_subfolder = os.path.join(log_subfolder, 'tensorboard')\n",
    "    if not os.path.exists(log_subfolder):  \n",
    "        os.makedirs(log_subfolder)\n",
    "    if not os.path.exists(tensorboard_log_subfolder):  \n",
    "        os.makedirs(tensorboard_log_subfolder)\n",
    "    log_writer = SummaryWriter(log_dir=tensorboard_log_subfolder)\n",
    "    \n",
    "    # add the model architecture as a graph\n",
    "    sample_state_batch, _ = next(iter(train_loader))\n",
    "    log_writer.add_graph(model, sample_state_batch)\n",
    "    \n",
    "    \n",
    "    best_model_path, best_val_loss, best_model_valid_accuracy, epochs_without_improvement = None, float('inf'), -1.0, 0\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    train_losses, valid_losses = [], []\n",
    "    \n",
    "    \n",
    "    for epoch in tqdm(range(epochs), desc='Epochs'):\n",
    "        model.train()\n",
    "        correct_train, total_train, train_loss = 0, 0, 0.0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        train_accuracy = correct_train / total_train\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        log_writer.add_scalar(\"Loss/Train\", avg_train_loss, epoch)\n",
    "        log_writer.add_scalar(\"Accuracy/Train\", train_accuracy, epoch)\n",
    "        \n",
    "        model.eval()\n",
    "        correct_val, total_val, valid_loss = 0, 0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_function(outputs, labels)\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "        val_accuracy = correct_val / total_val\n",
    "        avg_valid_loss = valid_loss/len(val_loader)\n",
    "        \n",
    "        log_writer.add_scalar(\"Loss/Valid\", avg_valid_loss, epoch)\n",
    "        log_writer.add_scalar(\"Accuracy/Valid\", val_accuracy, epoch)\n",
    "    \n",
    "        # average loss per batch\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        valid_losses.append(valid_loss / len(val_loader))\n",
    "        \n",
    "        # early stopping:\n",
    "        if valid_loss < best_val_loss:\n",
    "            #best performing model here, save it:\n",
    "            best_model_path = os.path.join(log_subfolder, f\"best_model.pt\")\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"Best model saved at epoch {epoch+1} with validation loss: {valid_loss/len(val_loader):.4f}\")\n",
    "            epochs_without_improvement = 0\n",
    "            best_val_loss = valid_loss\n",
    "            best_model_valid_accuracy = val_accuracy\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        if epochs_without_improvement >= early_stop_epoch_without_improvement:\n",
    "            break\n",
    "    \n",
    "    log_writer.flush()\n",
    "    log_writer.close()\n",
    "    \n",
    "    best_model = copy.deepcopy(model)\n",
    "    best_model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "    return best_model, best_model_valid_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BC(torch.nn.Module):\n",
    "    def __init__(self, input_neurons: int,\n",
    "                 hidden_neurons: int,\n",
    "                 num_hidden_layers: int,\n",
    "                 out_neurons: int,\n",
    "                 activation_function: torch.nn.Module = torch.nn.ReLU()):\n",
    "        super().__init__()\n",
    "\n",
    "        # Add the first (input) layer + activation function\n",
    "        layers = [torch.nn.Linear(input_neurons, hidden_neurons),\n",
    "                  activation_function]\n",
    "\n",
    "        # Add the hidden layers\n",
    "        for _ in range(num_hidden_layers):\n",
    "            layers.append(torch.nn.Linear(hidden_neurons, hidden_neurons))\n",
    "            layers.append(activation_function)\n",
    "\n",
    "        layers.append(torch.nn.Linear(hidden_neurons, out_neurons))\n",
    "\n",
    "        # Combine the layers into a container\n",
    "        self.network = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Testing </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0249, -0.0718,  0.0541,  0.0162], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc_model = BC(input_neurons=8, num_hidden_layers=128, hidden_neurons=256, out_neurons=4)\n",
    "bc_model(torch.Tensor(rb_observations[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader, valid_loader = get_BC_data_loaders(observations=rb_observations[0:1000], actions=rb_actions.flatten()[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12, Training Loss: 1.3928, Training Accuracy: 0.236052%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   8%|▊         | 1/12 [00:02<00:28,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12, Validation Loss: 1.3852, Validation Accuracy: 0.264000%\n",
      "Best model saved at epoch 1 with validation loss: 1.3852\n",
      "Epoch 2/12, Training Loss: 1.3858, Training Accuracy: 0.278970%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  17%|█▋        | 2/12 [00:05<00:25,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/12, Validation Loss: 1.3848, Validation Accuracy: 0.264000%\n",
      "Best model saved at epoch 2 with validation loss: 1.3848\n",
      "Epoch 3/12, Training Loss: 1.3860, Training Accuracy: 0.278970%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  25%|██▌       | 3/12 [00:07<00:22,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/12, Validation Loss: 1.3862, Validation Accuracy: 0.264000%\n",
      "Epoch 4/12, Training Loss: 1.3851, Training Accuracy: 0.278970%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  33%|███▎      | 4/12 [00:10<00:19,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/12, Validation Loss: 1.3860, Validation Accuracy: 0.264000%\n",
      "Epoch 5/12, Training Loss: 1.3846, Training Accuracy: 0.278970%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  33%|███▎      | 4/12 [00:12<00:24,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/12, Validation Loss: 1.3865, Validation Accuracy: 0.264000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(BC(\n",
       "   (network): Sequential(\n",
       "     (0): Linear(in_features=8, out_features=256, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (3): ReLU()\n",
       "     (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (5): ReLU()\n",
       "     (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (7): ReLU()\n",
       "     (8): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (9): ReLU()\n",
       "     (10): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (11): ReLU()\n",
       "     (12): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (13): ReLU()\n",
       "     (14): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (15): ReLU()\n",
       "     (16): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (17): ReLU()\n",
       "     (18): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (19): ReLU()\n",
       "     (20): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (21): ReLU()\n",
       "     (22): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (23): ReLU()\n",
       "     (24): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (25): ReLU()\n",
       "     (26): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (27): ReLU()\n",
       "     (28): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (29): ReLU()\n",
       "     (30): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (31): ReLU()\n",
       "     (32): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (33): ReLU()\n",
       "     (34): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (35): ReLU()\n",
       "     (36): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (37): ReLU()\n",
       "     (38): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (39): ReLU()\n",
       "     (40): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (41): ReLU()\n",
       "     (42): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (43): ReLU()\n",
       "     (44): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (45): ReLU()\n",
       "     (46): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (47): ReLU()\n",
       "     (48): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (49): ReLU()\n",
       "     (50): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (51): ReLU()\n",
       "     (52): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (53): ReLU()\n",
       "     (54): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (55): ReLU()\n",
       "     (56): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (57): ReLU()\n",
       "     (58): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (59): ReLU()\n",
       "     (60): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (61): ReLU()\n",
       "     (62): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (63): ReLU()\n",
       "     (64): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (65): ReLU()\n",
       "     (66): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (67): ReLU()\n",
       "     (68): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (69): ReLU()\n",
       "     (70): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (71): ReLU()\n",
       "     (72): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (73): ReLU()\n",
       "     (74): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (75): ReLU()\n",
       "     (76): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (77): ReLU()\n",
       "     (78): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (79): ReLU()\n",
       "     (80): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (81): ReLU()\n",
       "     (82): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (83): ReLU()\n",
       "     (84): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (85): ReLU()\n",
       "     (86): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (87): ReLU()\n",
       "     (88): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (89): ReLU()\n",
       "     (90): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (91): ReLU()\n",
       "     (92): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (93): ReLU()\n",
       "     (94): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (95): ReLU()\n",
       "     (96): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (97): ReLU()\n",
       "     (98): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (99): ReLU()\n",
       "     (100): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (101): ReLU()\n",
       "     (102): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (103): ReLU()\n",
       "     (104): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (105): ReLU()\n",
       "     (106): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (107): ReLU()\n",
       "     (108): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (109): ReLU()\n",
       "     (110): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (111): ReLU()\n",
       "     (112): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (113): ReLU()\n",
       "     (114): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (115): ReLU()\n",
       "     (116): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (117): ReLU()\n",
       "     (118): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (119): ReLU()\n",
       "     (120): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (121): ReLU()\n",
       "     (122): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (123): ReLU()\n",
       "     (124): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (125): ReLU()\n",
       "     (126): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (127): ReLU()\n",
       "     (128): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (129): ReLU()\n",
       "     (130): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (131): ReLU()\n",
       "     (132): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (133): ReLU()\n",
       "     (134): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (135): ReLU()\n",
       "     (136): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (137): ReLU()\n",
       "     (138): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (139): ReLU()\n",
       "     (140): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (141): ReLU()\n",
       "     (142): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (143): ReLU()\n",
       "     (144): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (145): ReLU()\n",
       "     (146): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (147): ReLU()\n",
       "     (148): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (149): ReLU()\n",
       "     (150): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (151): ReLU()\n",
       "     (152): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (153): ReLU()\n",
       "     (154): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (155): ReLU()\n",
       "     (156): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (157): ReLU()\n",
       "     (158): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (159): ReLU()\n",
       "     (160): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (161): ReLU()\n",
       "     (162): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (163): ReLU()\n",
       "     (164): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (165): ReLU()\n",
       "     (166): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (167): ReLU()\n",
       "     (168): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (169): ReLU()\n",
       "     (170): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (171): ReLU()\n",
       "     (172): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (173): ReLU()\n",
       "     (174): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (175): ReLU()\n",
       "     (176): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (177): ReLU()\n",
       "     (178): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (179): ReLU()\n",
       "     (180): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (181): ReLU()\n",
       "     (182): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (183): ReLU()\n",
       "     (184): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (185): ReLU()\n",
       "     (186): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (187): ReLU()\n",
       "     (188): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (189): ReLU()\n",
       "     (190): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (191): ReLU()\n",
       "     (192): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (193): ReLU()\n",
       "     (194): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (195): ReLU()\n",
       "     (196): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (197): ReLU()\n",
       "     (198): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (199): ReLU()\n",
       "     (200): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (201): ReLU()\n",
       "     (202): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (203): ReLU()\n",
       "     (204): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (205): ReLU()\n",
       "     (206): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (207): ReLU()\n",
       "     (208): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (209): ReLU()\n",
       "     (210): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (211): ReLU()\n",
       "     (212): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (213): ReLU()\n",
       "     (214): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (215): ReLU()\n",
       "     (216): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (217): ReLU()\n",
       "     (218): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (219): ReLU()\n",
       "     (220): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (221): ReLU()\n",
       "     (222): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (223): ReLU()\n",
       "     (224): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (225): ReLU()\n",
       "     (226): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (227): ReLU()\n",
       "     (228): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (229): ReLU()\n",
       "     (230): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (231): ReLU()\n",
       "     (232): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (233): ReLU()\n",
       "     (234): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (235): ReLU()\n",
       "     (236): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (237): ReLU()\n",
       "     (238): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (239): ReLU()\n",
       "     (240): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (241): ReLU()\n",
       "     (242): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (243): ReLU()\n",
       "     (244): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (245): ReLU()\n",
       "     (246): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (247): ReLU()\n",
       "     (248): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (249): ReLU()\n",
       "     (250): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (251): ReLU()\n",
       "     (252): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (253): ReLU()\n",
       "     (254): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (255): ReLU()\n",
       "     (256): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (257): ReLU()\n",
       "     (258): Linear(in_features=256, out_features=4, bias=True)\n",
       "   )\n",
       " ),\n",
       " 0.264)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_evaluate(train_loader=train_loader, val_loader=valid_loader,  model=bc_model,\n",
    "                optimizer = torch.optim.Adam(bc_model.parameters(), lr=0.01), epochs=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
